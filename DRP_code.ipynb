{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vffKI4hX5GR-"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import math as math\n",
    "import re\n",
    "\n",
    "MAX_ENTROPY = 0\n",
    "MAX_LENGTH = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (23.3.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zoxk7i3E1tp5"
   },
   "outputs": [],
   "source": [
    "# loading the data frame\n",
    "def load_dataframe(file):\n",
    "    df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n",
    "    df = shuffle(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HbzXHhDkCali"
   },
   "outputs": [],
   "source": [
    "# load train data and test data for Naive Bayes\n",
    "\n",
    "def NB_load_training_data(df, strength_class, train_size):\n",
    "    # Strength - three values(0 , 1 , 2) i.e. 0 for weak, 1 for medium, 2 for strong.\n",
    "    # 3 types of password 0,1,2\n",
    "    df_train = df.loc[df['strength'].isin(strength_class)]\n",
    "    df_train = df_train.head(train_size)\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def NB_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    #Strength - three values(0 , 1 , 2) i.e. 0 for weak, 1 for medium, 2 for strong.\n",
    "    # 3 types of password 0,1,2\n",
    "    df_test = df.loc[df['strength'].isin(strength_class)]\n",
    "    df_test = df_test.iloc[test_size_start:test_size_end]\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "bvMFTwsTCJao",
    "outputId": "8b2cb576-ed54-4645-8b92-e3c15d1dea2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/3bzjl2hs4178tvwhq5sftr840000gn/T/ipykernel_1131/1506715142.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n",
      "/var/folders/h7/3bzjl2hs4178tvwhq5sftr840000gn/T/ipykernel_1131/1506715142.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669640\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "classes = [0,1,2]\n",
    "test_size = 5000\n",
    "\n",
    "\n",
    "df = load_dataframe(\"data.csv\")\n",
    "print(len(df))\n",
    "df_train = NB_load_training_data(df,classes, train_size)\n",
    "df_test = NB_load_test_data(df , classes, train_size + 1,train_size + 1 + test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EtsdoXe3CC7s"
   },
   "outputs": [],
   "source": [
    "#Sklearn Naive Bayes Model\n",
    "number = preprocessing.LabelEncoder()\n",
    "\n",
    "df_train_sk = df_train.copy()\n",
    "df_test_sk = df_test.copy()\n",
    "\n",
    "# train\n",
    "df_train_sk['password'] = number.fit_transform(df_train_sk.password)\n",
    "X = df_train_sk['password']\n",
    "X = np.asarray(X)\n",
    "X_train = X.reshape(-1, 1)\n",
    "y_train = df_train_sk['strength']\n",
    "# test\n",
    "df_test_sk['password'] = number.fit_transform(df_test_sk.password)\n",
    "X = df_test_sk['password']\n",
    "X = np.asarray(X)\n",
    "X_test = X.reshape(-1, 1)\n",
    "y_test = df_test_sk['strength']\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-PDN4fomx6ag"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes From Scratch \n",
    "class NaiveBayes:\n",
    "   \n",
    "    def __init__(self, df, classes, train_size):\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        self.train_size = train_size\n",
    "        self.password_class_letter_dic_list = []\n",
    "        self.password_class_letters_list = []\n",
    "        self.password_class_p = []\n",
    "        self.all_letter_dic = {}\n",
    "        self.total_letters = 0\n",
    "        self.password_class_p = []\n",
    "     \n",
    "\n",
    "        for r in range(0,len(classes)):\n",
    "            password_dic,password_total_letters = self.letter_dic_generator(classes[r])\n",
    "            self.password_class_letter_dic_list.append(password_dic)\n",
    "            self.password_class_letters_list.append(password_total_letters)\n",
    "\n",
    "        self.all_letter_dic, self.total_letters = self.letter_dic_generator()\n",
    "        \n",
    "        for i in range(0,len(classes)):\n",
    "            self.password_class_p.append(self.password_type_counter(df,classes[i]))\n",
    "\n",
    "    def password_type_counter(self,df,strength):\n",
    "        return df.loc[df['strength'] == strength]['strength'].count()\n",
    "    \n",
    "    def letter_dic_generator(self,dic_strength = -1):\n",
    "        letter_dic = {}\n",
    "        total_letters = 0\n",
    "        for i in range(0, len(self.df)):\n",
    "            password = self.df.iloc[i]['password']\n",
    "            strength = str(self.df.iloc[i]['strength'])\n",
    "            if(strength == str(dic_strength) or dic_strength == -1):\n",
    "                for letter in password:\n",
    "                    if(letter in letter_dic):\n",
    "                        letter_dic[letter] += 1\n",
    "                    else:\n",
    "                        letter_dic[letter] = 1 \n",
    "                    total_letters += 1\n",
    "        return (letter_dic, total_letters)\n",
    "\n",
    "    def denominator_calc(self, password):\n",
    "        p_x = 0\n",
    "        for r in range(0,len(self.classes)):\n",
    "            y = 1 \n",
    "            for letter in password:\n",
    "                y = y  * (self.password_class_letter_dic_list[r][letter]/self.password_class_letters_list[r])\n",
    "            y = y * self.password_class_p[r]\n",
    "            p_x = y + p_x\n",
    "        return p_x\n",
    "        \n",
    "    def probability_calc (self, p,dic, total_letters_counter, password):\n",
    "        p_x = self.denominator_calc(password)\n",
    "        p_x_y = 1\n",
    "        for letter in password:\n",
    "            p_x_y = p_x_y * (dic[letter]/total_letters_counter)\n",
    "        p_y_x = p_x_y * p / p_x\n",
    "        return p_y_x\n",
    "\n",
    "    def password_strength_checker(self, password):\n",
    "        prob = []\n",
    "        for r in range(0,len(self.classes)):\n",
    "            p = self.probability_calc(self.password_class_p[r],self.password_class_letter_dic_list[r],self.password_class_letters_list[r],password)\n",
    "            prob.append(p)\n",
    "        \n",
    "        max_value = max(prob)\n",
    "        max_index = prob.index(max_value)\n",
    "        return self.classes[max_index]\n",
    "    \n",
    "    def accuracy(self,df_test):\n",
    "        df_test_num_counter = 0\n",
    "        accuracy = 0 \n",
    "        for i in range(0, len(df_test)):\n",
    "            try:\n",
    "                password = df_test.iloc[i]['password']\n",
    "                strength = str(df_test.iloc[i]['strength'])\n",
    "                output = self.password_strength_checker(password)\n",
    "                if(str(output) == strength):\n",
    "                    accuracy = accuracy + 1\n",
    "                df_test_num_counter += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return (accuracy/df_test_num_counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "APZ0Wl6yANrf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Scratch Naive Bayes:0.820898731455601\n",
      "Accuracy of Sklearn Naive Bayes model is: 0.742600 \n"
     ]
    }
   ],
   "source": [
    "# Comparing the two Naive Bates Models models\n",
    "NaiveBayesClass = NaiveBayes(df_train, classes, train_size)\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "accuracy = ((y_test == y_pred).sum())/test_size\n",
    "\n",
    "print(\"Accuracy of Scratch Naive Bayes:\" + str(NaiveBayesClass.accuracy(df_test)))\n",
    "print(\"Accuracy of Sklearn Naive Bayes model is: %f \" % (accuracy))\n",
    "#print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(string):\n",
    "    \"Calculates the Shannon entropy of a string\"\n",
    "\n",
    "    uppercase = False\n",
    "    lowercase = False\n",
    "    digit = False\n",
    "    specialchar = False\n",
    "    for char in string:\n",
    "        k = char.islower() \n",
    "        c = char.isupper() \n",
    "        if(k == True ):\n",
    "             lowercase = True\n",
    "        elif(c==True):\n",
    "            uppercase = True\n",
    "        elif(char.isnumeric()):\n",
    "            digit = True\n",
    "        else:\n",
    "            specialchar = True\n",
    "    p = 0\n",
    "    if(uppercase):\n",
    "        p = p + 26\n",
    "    if(lowercase):\n",
    "        p = p + 26\n",
    "    if(digit):\n",
    "        p = p + 10\n",
    "    if(specialchar):\n",
    "        p = p + 30\n",
    "    entropy = (math.log(p**len(string)) / math.log(2.0))\n",
    "    return entropy\n",
    "\n",
    "#https://generatepasswords.org/how-to-calculate-entropy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_load_train_data(df, strength_class, train_size):\n",
    "    df_train = df.head(train_size)\n",
    "    df_train = df_train.drop('strength', axis=1)\n",
    "    \n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    global MAX_ENTROPY \n",
    "    MAX_ENTROPY = 0\n",
    "    global MAX_LENGTH \n",
    "    MAX_LENGTH = 0 \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        if(entropy(password) > MAX_ENTROPY):\n",
    "            MAX_ENTROPY =  entropy(password) \n",
    "        if(len(password) > MAX_LENGTH):\n",
    "            MAX_LENGTH = len(password)\n",
    "    \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        \n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "    \n",
    "    df_train.insert(1, 'password_length',password_length_ary)\n",
    "    df_train.insert(2,'entropy',entropy_list)\n",
    "    return df_train\n",
    "\n",
    "def KMeans_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    df_test_temp = df.iloc[test_size_start:test_size_end]\n",
    "    df_test = df_test_temp.loc[df['strength'].isin(strength_class)]\n",
    "\n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    for passwordIndex in range(0,len(df_test)):\n",
    "        password = df_test.iloc[passwordIndex]['password']\n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "        \n",
    "    df_test.insert(1, 'password_length',password_length_ary)\n",
    "    df_test.insert(2,'entropy',entropy_list)\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def KMeans_sklearn_data_generator(df):\n",
    "    KMeans_sklearn_data = []\n",
    "    for r in range(0,len(df)):\n",
    "        password = df.iloc[r]\n",
    "        point = [float(password['password_length']),float(password['entropy'])]\n",
    "        KMeans_sklearn_data.append(point)\n",
    "    return  (KMeans_sklearn_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClass:\n",
    "    def __init__(self, df_train,num_classes,num_features):\n",
    "        self.df_train = df_train\n",
    "        self.num_classes = num_classes\n",
    "        self.final_centroids = []\n",
    "        self.num_features = num_features\n",
    "    \n",
    "        self.centroids_cluster_counter = [0] * num_classes\n",
    "        \n",
    "    def intial_centriod_generator(self):\n",
    "        distances = []\n",
    "        intial_centroids = []\n",
    "        rand_values = []\n",
    "  \n",
    "        for r in range(0, self.num_classes):\n",
    "            while(True):\n",
    "                random_index = random.randint(0, len(df_train)-1)\n",
    "                if(random_index not in rand_values):\n",
    "                    rand_centroid = self.df_train.iloc[random_index]\n",
    "                    point = []\n",
    "                    point.append(float(rand_centroid['password_length']))\n",
    "                    point.append(float(rand_centroid['entropy']))\n",
    "                    \n",
    "                    if(point not in intial_centroids):\n",
    "                        intial_centroids.append(point)\n",
    "                        rand_values.append(random_index)\n",
    "                        break\n",
    "        \n",
    "        intial_centroids = sorted(intial_centroids, key=sum)\n",
    "        return intial_centroids\n",
    "    \n",
    "    def model_generator(self, max_itteration, centroids):\n",
    "        number_itteration = 0\n",
    "        group = []\n",
    "\n",
    "        while(True):\n",
    "            number_itteration = number_itteration + 1\n",
    "            temp_group = group\n",
    "            group = []\n",
    "            centroids_itter_sum = [[0,0],[0,0],[0,0]] \n",
    "            centroids_points_sum_counter = [0,0,0]\n",
    "\n",
    "            for passwordIndex in range(0,len(df_train)):\n",
    "                distances = []\n",
    "                \n",
    "                for centroid_counter in range(len(centroids)):\n",
    "                    password = df_train.iloc[passwordIndex]\n",
    "                    distance_calc = math.sqrt(((float(password['password_length'])-float(centroids[centroid_counter][0]))**2) +((float(password['entropy'])-float(centroids[centroid_counter][1]))**2))\n",
    "                    distances.append([distance_calc,centroid_counter])\n",
    "                \n",
    "                min_diststance = min(distances, key=lambda x: x[0])\n",
    "\n",
    "                centroids_points_sum_counter[min_diststance[1]] += 1\n",
    "                                               \n",
    "                centroids_itter_sum[min_diststance[1]][0] += password['password_length']\n",
    "                centroids_itter_sum[min_diststance[1]][1] += password['entropy']\n",
    "                \n",
    "                group.append(min_diststance[1])\n",
    "\n",
    "            \n",
    "            for centroid_counter in range(0,self.num_classes):\n",
    "                new_point = []\n",
    "                new_point.append(centroids_itter_sum[centroid_counter][0] / centroids_points_sum_counter[centroid_counter])\n",
    "                new_point.append(centroids_itter_sum[centroid_counter][1] / centroids_points_sum_counter[centroid_counter])\n",
    "\n",
    "                centroids[centroid_counter] = new_point\n",
    "                \n",
    "\n",
    "            if(number_itteration >= max_itteration):\n",
    "                change = False\n",
    "                df_train.insert(self.num_features , 'predicted_strength',group)\n",
    "                return centroids\n",
    "            if(temp_group == group):\n",
    "                change = False\n",
    "                df_train.insert(self.num_features , 'predicted_strength',group)\n",
    "                \n",
    "                return centroids\n",
    "\n",
    "    def strength_pred(self,password):\n",
    "        distances = []\n",
    "        for r in range(len(self.final_centroids)):\n",
    "            distance_calc = math.sqrt((float(password['password_length'])-self.final_centroids[r][0])**2 +(float(password['entropy'])-self.final_centroids[r][1])**2)\n",
    "            distances.append([distance_calc,r])\n",
    "        min_diststance = min(distances, key=lambda x: x[0])\n",
    "        pred_strength = min_diststance[1] \n",
    "        return pred_strength\n",
    "\n",
    "    def accuracy(self,df_test,test_size):\n",
    "        accuracy = 0\n",
    "        for passwordIndex in range(0,test_size):\n",
    "            password = df_test.iloc[passwordIndex]\n",
    "            #print(self.strength_pred(password))\n",
    "            if(self.strength_pred(password) == password['strength']):\n",
    "                accuracy = accuracy + 1\n",
    "        return accuracy/test_size\n",
    "        \n",
    "    def fit(self, max_itteration):\n",
    "        centroids = self.intial_centriod_generator()\n",
    "        self.final_centroids = self.model_generator(max_itteration,centroids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = 50\n",
    "classes = [0,1,2]\n",
    "test_size = 1000\n",
    "max_itteration = 500\n",
    "\n",
    "df_train = KMeans_load_train_data(df,classes, train_size)\n",
    "df_test = KMeans_load_test_data(df , classes, train_size + 1,train_size + 1 + test_size)\n",
    "\n",
    "\n",
    "\n",
    "KMeansModel= KMeansClass(df_train, len(classes), 3)\n",
    "KMeansModel.fit(max_itteration)\n",
    "#print(KMeansModel.final_centroids)\n",
    "print(KMeansModel.accuracy(df_test,test_size))\n",
    "\n",
    "\n",
    "\n",
    "X_train_KMeans_sklearn = KMeans_sklearn_data_generator(df_train)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(X_train_KMeans_sklearn)\n",
    "#print(kmeans.cluster_centers_)\n",
    "\n",
    "X_test = KMeans_sklearn_data_generator(df_test)\n",
    "pred_kmeans_sklearn = (kmeans.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>password_length</th>\n",
       "      <th>entropy</th>\n",
       "      <th>predicted_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245237</th>\n",
       "      <td>1568316ns</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.445781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363514</th>\n",
       "      <td>2263508a</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.396250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360254</th>\n",
       "      <td>wvxhjcz328</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.495313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183330</th>\n",
       "      <td>ciquwic916</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.495313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420224</th>\n",
       "      <td>wei43333556</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.544844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           password  password_length   entropy  predicted_strength\n",
       "245237    1568316ns           0.5625  0.445781                   1\n",
       "363514     2263508a           0.5000  0.396250                   0\n",
       "360254   wvxhjcz328           0.6250  0.495313                   1\n",
       "183330   ciquwic916           0.6250  0.495313                   1\n",
       "420224  wei43333556           0.6875  0.544844                   1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_load_train_data(df, strength_class, train_size):\n",
    "    df_train = df.head(train_size)\n",
    "    \n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    global MAX_ENTROPY\n",
    "    MAX_ENTROPY = 0\n",
    "    global MAX_LENGTH \n",
    "    MAX_LENGTH = 0 \n",
    "    \n",
    "    for passwordIndex in range(0,len(df_train)):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        if(entropy(password) > MAX_ENTROPY):\n",
    "            MAX_ENTROPY =  entropy(password) \n",
    "        if(len(password) > MAX_LENGTH):\n",
    "            MAX_LENGTH = len(password)\n",
    "            \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        \n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "    \n",
    "    df_train.insert(1, 'password_length',password_length_ary)\n",
    "    df_train.insert(2,'entropy',entropy_list)\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "def NN_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    df_test_temp = df.iloc[test_size_start:test_size_end]\n",
    "    df_test = df_test_temp.loc[df['strength'].isin(strength_class)]\n",
    "\n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    for passwordIndex in range(0,len(df_test)):\n",
    "        password = df_test.iloc[passwordIndex]['password']\n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "        \n",
    "    df_test.insert(1, 'password_length',password_length_ary)\n",
    "    df_test.insert(2,'entropy',entropy_list)\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClass:\n",
    "    def __init__(self):\n",
    "        self.NN = None\n",
    "        #self.NNDef = None\n",
    "        \n",
    "    def model_generator(self,df_train,neuron_num_layer1,neuron_num_layer2):\n",
    "    \n",
    "        columns = [\"password_length\",\"entropy\"]\n",
    "        y_train = df_train[\"strength\"]\n",
    "        df_train = df_train.drop([\"strength\"], axis=1)\n",
    "\n",
    "        df_train['password'] = number.fit_transform(df_train.password)\n",
    "\n",
    "        for feature in columns:\n",
    "            le =  preprocessing.LabelEncoder()\n",
    "            df_train[feature] = le.fit_transform(df_train[feature])\n",
    "        X_train = df_train.to_numpy()\n",
    "        \n",
    "        self.NN =  MLPClassifier(hidden_layer_sizes=(neuron_num_layer1,neuron_num_layer2))\n",
    "        #self.NNDef =  MLPClassifier()\n",
    "        self.NN = self.NN.fit(X_train,y_train)\n",
    "        #self.NNDef = self.NNDef.fit(X_train,y_train)\n",
    "            \n",
    "    def accuracy(self, df_test):\n",
    "        df_test['password'] = number.fit_transform(df_test.password)\n",
    "        y_test = df_test[\"strength\"]\n",
    "        df_test = df_test.drop([\"strength\"], axis=1)\n",
    "        columns = [\"password_length\",\"entropy\"]\n",
    "        for feature in columns:\n",
    "            le =  preprocessing.LabelEncoder()\n",
    "            df_test[feature] = le.fit_transform(df_test[feature])\n",
    "        X_test = df_test.to_numpy() \n",
    "        y_pred = self.NN.predict(X_test)\n",
    "        \n",
    "        #y_predDef = self.NNDef.predict(X_test)\n",
    "            \n",
    "        accuracy = accuracy_score(y_test,y_pred) * 100\n",
    "        #accuracyDef = accuracy_score(y_test,y_predDef) * 100\n",
    "        #print(\"Default Accuracy\")\n",
    "        #print(accuracyDef)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jV9onC4oytIg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/3bzjl2hs4178tvwhq5sftr840000gn/T/ipykernel_1131/2529140562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# normalize everything between 0 and 1 by dividing by max size possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# letter frequnecy of each password\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = 2000\n",
    "df_train = NN_load_train_data(df,classes, train_size)\n",
    "test_size = 1000\n",
    "df_test = NN_load_test_data(df , classes, train_size+1,train_size+1+test_size)\n",
    "\n",
    "accuracy_list = []\n",
    "\n",
    "password_dic_length = {}\n",
    "entropy_dic = {}\n",
    "for r in range(0,len(df_train)):\n",
    "    password  = df_train.iloc[r]\n",
    "    if(password[\"entropy\"] not in entropy_dic ):\n",
    "        entropy_dic[password[\"entropy\"]] =password[\"entropy\"]\n",
    "    if(len(password[\"password\"]) not in password_dic_length):\n",
    "        password_dic_length[len(password[\"password\"])] = len(password[\"password\"])\n",
    "\n",
    "#print(len(entropy_dic))\n",
    "#print(len(password_dic_length))\n",
    "                                                             \n",
    "NNModel = NNClass()\n",
    "\n",
    "for layer1_neruon_count in range(1,100,10):\n",
    "    for layer2_neruon_count in range(0,100,10):\n",
    "        NNModel.model_generator(df_train,len(entropy_dic),len(password_dic_length))\n",
    "        acr = NNModel.accuracy(df_test)\n",
    "        accuracy_list.append(acr)\n",
    "        #print(NNModel.accuracy(df_test))\n",
    "\n",
    "# try different numbers of layers and neurons\n",
    "# store the data\n",
    "# make plots for analysis\n",
    "# time length of training\n",
    "# normalize everything between 0 and 1 by dividing by max size possible\n",
    "# letter frequnecy of each password\n",
    "print(max(acr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWNjqQRX3073"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9bSOW0o329X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD37lYBoyRpd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0yIcO9Jx20K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuNXa_gEyaQf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
