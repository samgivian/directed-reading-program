{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vffKI4hX5GR-"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "import math as math\n",
    "import re\n",
    "\n",
    "MAX_ENTROPY = 0\n",
    "MAX_LENGTH = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (23.3.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/samangivian/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zoxk7i3E1tp5"
   },
   "outputs": [],
   "source": [
    "# loading the data frame\n",
    "def load_dataframe(file):\n",
    "    df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n",
    "    df = shuffle(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HbzXHhDkCali"
   },
   "outputs": [],
   "source": [
    "# load train data and test data for Naive Bayes\n",
    "\n",
    "def NB_load_training_data(df, strength_class, train_size):\n",
    "    # Strength - three values(0 , 1 , 2) i.e. 0 for weak, 1 for medium, 2 for strong.\n",
    "    # 3 types of password 0,1,2\n",
    "    df_train = df.loc[df['strength'].isin(strength_class)]\n",
    "    df_train = df_train.head(train_size)\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def NB_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    #Strength - three values(0 , 1 , 2) i.e. 0 for weak, 1 for medium, 2 for strong.\n",
    "    # 3 types of password 0,1,2\n",
    "    df_test = df.loc[df['strength'].isin(strength_class)]\n",
    "    df_test = df_test.iloc[test_size_start:test_size_end]\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "bvMFTwsTCJao",
    "outputId": "8b2cb576-ed54-4645-8b92-e3c15d1dea2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/3bzjl2hs4178tvwhq5sftr840000gn/T/ipykernel_1814/1506715142.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n",
      "/var/folders/h7/3bzjl2hs4178tvwhq5sftr840000gn/T/ipykernel_1814/1506715142.py:3: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(file, error_bad_lines = False, warn_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "669640\n"
     ]
    }
   ],
   "source": [
    "train_size = 10000\n",
    "classes = [0,1,2]\n",
    "test_size = 5000\n",
    "\n",
    "\n",
    "df = load_dataframe(\"data.csv\")\n",
    "print(len(df))\n",
    "df_train = NB_load_training_data(df,classes, train_size)\n",
    "df_test = NB_load_test_data(df , classes, train_size + 1,train_size + 1 + test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EtsdoXe3CC7s"
   },
   "outputs": [],
   "source": [
    "#Sklearn Naive Bayes Model\n",
    "number = preprocessing.LabelEncoder()\n",
    "\n",
    "df_train_sk = df_train.copy()\n",
    "df_test_sk = df_test.copy()\n",
    "\n",
    "# train\n",
    "df_train_sk['password'] = number.fit_transform(df_train_sk.password)\n",
    "X = df_train_sk['password']\n",
    "X = np.asarray(X)\n",
    "X_train = X.reshape(-1, 1)\n",
    "y_train = df_train_sk['strength']\n",
    "# test\n",
    "df_test_sk['password'] = number.fit_transform(df_test_sk.password)\n",
    "X = df_test_sk['password']\n",
    "X = np.asarray(X)\n",
    "X_test = X.reshape(-1, 1)\n",
    "y_test = df_test_sk['strength']\n",
    "\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-PDN4fomx6ag"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes From Scratch \n",
    "class NaiveBayes:\n",
    "   \n",
    "    def __init__(self, df, classes, train_size):\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        self.train_size = train_size\n",
    "        self.password_class_letter_dic_list = []\n",
    "        self.password_class_letters_list = []\n",
    "        self.password_class_p = []\n",
    "        self.all_letter_dic = {}\n",
    "        self.total_letters = 0\n",
    "        self.password_class_p = []\n",
    "     \n",
    "\n",
    "        for r in range(0,len(classes)):\n",
    "            password_dic,password_total_letters = self.letter_dic_generator(classes[r])\n",
    "            self.password_class_letter_dic_list.append(password_dic)\n",
    "            self.password_class_letters_list.append(password_total_letters)\n",
    "\n",
    "        self.all_letter_dic, self.total_letters = self.letter_dic_generator()\n",
    "        \n",
    "        for i in range(0,len(classes)):\n",
    "            self.password_class_p.append(self.password_type_counter(df,classes[i]))\n",
    "\n",
    "    def password_type_counter(self,df,strength):\n",
    "        return df.loc[df['strength'] == strength]['strength'].count()\n",
    "    \n",
    "    def letter_dic_generator(self,dic_strength = -1):\n",
    "        letter_dic = {}\n",
    "        total_letters = 0\n",
    "        for i in range(0, len(self.df)):\n",
    "            password = self.df.iloc[i]['password']\n",
    "            strength = str(self.df.iloc[i]['strength'])\n",
    "            if(strength == str(dic_strength) or dic_strength == -1):\n",
    "                for letter in password:\n",
    "                    if(letter in letter_dic):\n",
    "                        letter_dic[letter] += 1\n",
    "                    else:\n",
    "                        letter_dic[letter] = 1 \n",
    "                    total_letters += 1\n",
    "        return (letter_dic, total_letters)\n",
    "\n",
    "    def denominator_calc(self, password):\n",
    "        p_x = 0\n",
    "        for r in range(0,len(self.classes)):\n",
    "            y = 1 \n",
    "            for letter in password:\n",
    "                y = y  * (self.password_class_letter_dic_list[r][letter]/self.password_class_letters_list[r])\n",
    "            y = y * self.password_class_p[r]\n",
    "            p_x = y + p_x\n",
    "        return p_x\n",
    "        \n",
    "    def probability_calc (self, p,dic, total_letters_counter, password):\n",
    "        p_x = self.denominator_calc(password)\n",
    "        p_x_y = 1\n",
    "        for letter in password:\n",
    "            p_x_y = p_x_y * (dic[letter]/total_letters_counter)\n",
    "        p_y_x = p_x_y * p / p_x\n",
    "        return p_y_x\n",
    "\n",
    "    def password_strength_checker(self, password):\n",
    "        prob = []\n",
    "        for r in range(0,len(self.classes)):\n",
    "            p = self.probability_calc(self.password_class_p[r],self.password_class_letter_dic_list[r],self.password_class_letters_list[r],password)\n",
    "            prob.append(p)\n",
    "        \n",
    "        max_value = max(prob)\n",
    "        max_index = prob.index(max_value)\n",
    "        return self.classes[max_index]\n",
    "    \n",
    "    def accuracy(self,df_test):\n",
    "        df_test_num_counter = 0\n",
    "        accuracy = 0 \n",
    "        for i in range(0, len(df_test)):\n",
    "            try:\n",
    "                password = df_test.iloc[i]['password']\n",
    "                strength = str(df_test.iloc[i]['strength'])\n",
    "                output = self.password_strength_checker(password)\n",
    "                if(str(output) == strength):\n",
    "                    accuracy = accuracy + 1\n",
    "                df_test_num_counter += 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return (accuracy/df_test_num_counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "APZ0Wl6yANrf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Scratch Naive Bayes:0.8285136501516683\n",
      "Accuracy of Sklearn Naive Bayes model is: 0.728000 \n"
     ]
    }
   ],
   "source": [
    "# Comparing the two Naive Bates Models models\n",
    "NaiveBayesClass = NaiveBayes(df_train, classes, train_size)\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "accuracy = ((y_test == y_pred).sum())/test_size\n",
    "\n",
    "print(\"Accuracy of Scratch Naive Bayes:\" + str(NaiveBayesClass.accuracy(df_test)))\n",
    "print(\"Accuracy of Sklearn Naive Bayes model is: %f \" % (accuracy))\n",
    "#print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(string):\n",
    "    \"Calculates the Shannon entropy of a string\"\n",
    "\n",
    "    uppercase = False\n",
    "    lowercase = False\n",
    "    digit = False\n",
    "    specialchar = False\n",
    "    for char in string:\n",
    "        k = char.islower() \n",
    "        c = char.isupper() \n",
    "        if(k == True ):\n",
    "             lowercase = True\n",
    "        elif(c==True):\n",
    "            uppercase = True\n",
    "        elif(char.isnumeric()):\n",
    "            digit = True\n",
    "        else:\n",
    "            specialchar = True\n",
    "    p = 0\n",
    "    if(uppercase):\n",
    "        p = p + 26\n",
    "    if(lowercase):\n",
    "        p = p + 26\n",
    "    if(digit):\n",
    "        p = p + 10\n",
    "    if(specialchar):\n",
    "        p = p + 30\n",
    "    entropy = (math.log(p**len(string)) / math.log(2.0))\n",
    "    return entropy\n",
    "\n",
    "#https://generatepasswords.org/how-to-calculate-entropy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_load_train_data(df, strength_class, train_size):\n",
    "    df_train = df.head(train_size)\n",
    "    df_train = df_train.drop('strength', axis=1)\n",
    "    \n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    global MAX_ENTROPY \n",
    "    MAX_ENTROPY = 0\n",
    "    global MAX_LENGTH \n",
    "    MAX_LENGTH = 0 \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        if(entropy(password) > MAX_ENTROPY):\n",
    "            MAX_ENTROPY =  entropy(password) \n",
    "        if(len(password) > MAX_LENGTH):\n",
    "            MAX_LENGTH = len(password)\n",
    "    \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        \n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "    \n",
    "    df_train.insert(1, 'password_length',password_length_ary)\n",
    "    df_train.insert(2,'entropy',entropy_list)\n",
    "    return df_train\n",
    "\n",
    "def KMeans_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    df_test_temp = df.iloc[test_size_start:test_size_end]\n",
    "    df_test = df_test_temp.loc[df['strength'].isin(strength_class)]\n",
    "\n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    for passwordIndex in range(0,len(df_test)):\n",
    "        password = df_test.iloc[passwordIndex]['password']\n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "        \n",
    "    df_test.insert(1, 'password_length',password_length_ary)\n",
    "    df_test.insert(2,'entropy',entropy_list)\n",
    "\n",
    "    return df_test\n",
    "\n",
    "\n",
    "def KMeans_sklearn_data_generator(df):\n",
    "    KMeans_sklearn_data = []\n",
    "    for r in range(0,len(df)):\n",
    "        password = df.iloc[r]\n",
    "        point = [float(password['password_length']),float(password['entropy'])]\n",
    "        KMeans_sklearn_data.append(point)\n",
    "    return  (KMeans_sklearn_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClass:\n",
    "    def __init__(self, df_train,num_classes,num_features):\n",
    "        self.df_train = df_train\n",
    "        self.num_classes = num_classes\n",
    "        self.final_centroids = []\n",
    "        self.num_features = num_features\n",
    "    \n",
    "        self.centroids_cluster_counter = [0] * num_classes\n",
    "        \n",
    "    def intial_centriod_generator(self):\n",
    "        distances = []\n",
    "        intial_centroids = []\n",
    "        rand_values = []\n",
    "  \n",
    "        for r in range(0, self.num_classes):\n",
    "            while(True):\n",
    "                random_index = random.randint(0, len(df_train)-1)\n",
    "                if(random_index not in rand_values):\n",
    "                    rand_centroid = self.df_train.iloc[random_index]\n",
    "                    point = []\n",
    "                    point.append(float(rand_centroid['password_length']))\n",
    "                    point.append(float(rand_centroid['entropy']))\n",
    "                    \n",
    "                    if(point not in intial_centroids):\n",
    "                        intial_centroids.append(point)\n",
    "                        rand_values.append(random_index)\n",
    "                        break\n",
    "        \n",
    "        intial_centroids = sorted(intial_centroids, key=sum)\n",
    "        return intial_centroids\n",
    "    \n",
    "    def model_generator(self, max_itteration, centroids):\n",
    "        number_itteration = 0\n",
    "        group = []\n",
    "\n",
    "        while(True):\n",
    "            number_itteration = number_itteration + 1\n",
    "            temp_group = group\n",
    "            group = []\n",
    "            centroids_itter_sum = [[0,0],[0,0],[0,0]] \n",
    "            centroids_points_sum_counter = [0,0,0]\n",
    "\n",
    "            for passwordIndex in range(0,len(df_train)):\n",
    "                distances = []\n",
    "                \n",
    "                for centroid_counter in range(len(centroids)):\n",
    "                    password = df_train.iloc[passwordIndex]\n",
    "                    distance_calc = math.sqrt(((float(password['password_length'])-float(centroids[centroid_counter][0]))**2) +((float(password['entropy'])-float(centroids[centroid_counter][1]))**2))\n",
    "                    distances.append([distance_calc,centroid_counter])\n",
    "                \n",
    "                min_diststance = min(distances, key=lambda x: x[0])\n",
    "\n",
    "                centroids_points_sum_counter[min_diststance[1]] += 1\n",
    "                                               \n",
    "                centroids_itter_sum[min_diststance[1]][0] += password['password_length']\n",
    "                centroids_itter_sum[min_diststance[1]][1] += password['entropy']\n",
    "                \n",
    "                group.append(min_diststance[1])\n",
    "\n",
    "            \n",
    "            for centroid_counter in range(0,self.num_classes):\n",
    "                new_point = []\n",
    "                new_point.append(centroids_itter_sum[centroid_counter][0] / centroids_points_sum_counter[centroid_counter])\n",
    "                new_point.append(centroids_itter_sum[centroid_counter][1] / centroids_points_sum_counter[centroid_counter])\n",
    "\n",
    "                centroids[centroid_counter] = new_point\n",
    "                \n",
    "\n",
    "            if(number_itteration >= max_itteration):\n",
    "                change = False\n",
    "                df_train.insert(self.num_features , 'predicted_strength',group)\n",
    "                return centroids\n",
    "            if(temp_group == group):\n",
    "                change = False\n",
    "                df_train.insert(self.num_features , 'predicted_strength',group)\n",
    "                \n",
    "                return centroids\n",
    "\n",
    "    def strength_pred(self,password):\n",
    "        distances = []\n",
    "        for r in range(len(self.final_centroids)):\n",
    "            distance_calc = math.sqrt((float(password['password_length'])-self.final_centroids[r][0])**2 +(float(password['entropy'])-self.final_centroids[r][1])**2)\n",
    "            distances.append([distance_calc,r])\n",
    "        min_diststance = min(distances, key=lambda x: x[0])\n",
    "        pred_strength = min_diststance[1] \n",
    "        return pred_strength\n",
    "\n",
    "    def accuracy(self,df_test,test_size):\n",
    "        accuracy = 0\n",
    "        for passwordIndex in range(0,test_size):\n",
    "            password = df_test.iloc[passwordIndex]\n",
    "            #print(self.strength_pred(password))\n",
    "            if(self.strength_pred(password) == password['strength']):\n",
    "                accuracy = accuracy + 1\n",
    "        return accuracy/test_size\n",
    "        \n",
    "    def fit(self, max_itteration):\n",
    "        centroids = self.intial_centriod_generator()\n",
    "        self.final_centroids = self.model_generator(max_itteration,centroids)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_size = 50\n",
    "classes = [0,1,2]\n",
    "test_size = 1000\n",
    "max_itteration = 500\n",
    "\n",
    "df_train = KMeans_load_train_data(df,classes, train_size)\n",
    "df_test = KMeans_load_test_data(df , classes, train_size + 1,train_size + 1 + test_size)\n",
    "\n",
    "\n",
    "\n",
    "KMeansModel= KMeansClass(df_train, len(classes), 3)\n",
    "KMeansModel.fit(max_itteration)\n",
    "#print(KMeansModel.final_centroids)\n",
    "print(KMeansModel.accuracy(df_test,test_size))\n",
    "\n",
    "\n",
    "\n",
    "X_train_KMeans_sklearn = KMeans_sklearn_data_generator(df_train)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(X_train_KMeans_sklearn)\n",
    "#print(kmeans.cluster_centers_)\n",
    "\n",
    "X_test = KMeans_sklearn_data_generator(df_test)\n",
    "pred_kmeans_sklearn = (kmeans.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>password</th>\n",
       "      <th>password_length</th>\n",
       "      <th>entropy</th>\n",
       "      <th>predicted_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27058</th>\n",
       "      <td>epwdlbxt03</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.427663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160372</th>\n",
       "      <td>009tf5ka0w</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.427663</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332061</th>\n",
       "      <td>mehdiaziz123</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.513195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182028</th>\n",
       "      <td>fecil30</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390496</th>\n",
       "      <td>goss88</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.256598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            password  password_length   entropy  predicted_strength\n",
       "27058     epwdlbxt03             0.50  0.427663                   0\n",
       "160372    009tf5ka0w             0.50  0.427663                   0\n",
       "332061  mehdiaziz123             0.60  0.513195                   1\n",
       "182028       fecil30             0.35  0.299364                   0\n",
       "390496        goss88             0.30  0.256598                   0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_load_train_data(df, strength_class, train_size):\n",
    "    df_train = df.head(train_size)\n",
    "    \n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    global MAX_ENTROPY\n",
    "    MAX_ENTROPY = 0\n",
    "    global MAX_LENGTH \n",
    "    MAX_LENGTH = 0 \n",
    "    \n",
    "    for passwordIndex in range(0,len(df_train)):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        if(entropy(password) > MAX_ENTROPY):\n",
    "            MAX_ENTROPY =  entropy(password) \n",
    "        if(len(password) > MAX_LENGTH):\n",
    "            MAX_LENGTH = len(password)\n",
    "            \n",
    "    for passwordIndex in range(0,train_size):\n",
    "        password = df_train.iloc[passwordIndex]['password']\n",
    "        \n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "    \n",
    "    df_train.insert(1, 'password_length',password_length_ary)\n",
    "    df_train.insert(2,'entropy',entropy_list)\n",
    "    \n",
    "    return df_train\n",
    "\n",
    "def NN_load_test_data(df, strength_class, test_size_start, test_size_end):\n",
    "    df_test_temp = df.iloc[test_size_start:test_size_end]\n",
    "    df_test = df_test_temp.loc[df['strength'].isin(strength_class)]\n",
    "\n",
    "    password_length_ary = []\n",
    "    entropy_list = []\n",
    "    \n",
    "    for passwordIndex in range(0,len(df_test)):\n",
    "        password = df_test.iloc[passwordIndex]['password']\n",
    "        password_length_ary.append(len(password)/MAX_LENGTH)\n",
    "        entropy_list.append(entropy(password)/MAX_ENTROPY)\n",
    "        \n",
    "    df_test.insert(1, 'password_length',password_length_ary)\n",
    "    df_test.insert(2,'entropy',entropy_list)\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClass:\n",
    "    def __init__(self):\n",
    "        self.NN = None\n",
    "        #self.NNDef = None\n",
    "        \n",
    "    def model_generator(self,df_train,neuron_num_layer1,neuron_num_layer2):\n",
    "    \n",
    "        columns = [\"password_length\",\"entropy\"]\n",
    "        y_train = df_train[\"strength\"]\n",
    "        df_train = df_train.drop([\"strength\"], axis=1)\n",
    "\n",
    "        df_train['password'] = number.fit_transform(df_train.password)\n",
    "\n",
    "        for feature in columns:\n",
    "            le =  preprocessing.LabelEncoder()\n",
    "            df_train[feature] = le.fit_transform(df_train[feature])\n",
    "        X_train = df_train.to_numpy()\n",
    "        \n",
    "        self.NN =  MLPClassifier(hidden_layer_sizes=(neuron_num_layer1,neuron_num_layer2))\n",
    "        #self.NNDef =  MLPClassifier()\n",
    "        self.NN = self.NN.fit(X_train,y_train)\n",
    "        #self.NNDef = self.NNDef.fit(X_train,y_train)\n",
    "            \n",
    "    def accuracy(self, df_test):\n",
    "        df_test['password'] = number.fit_transform(df_test.password)\n",
    "        y_test = df_test[\"strength\"]\n",
    "        df_test = df_test.drop([\"strength\"], axis=1)\n",
    "        columns = [\"password_length\",\"entropy\"]\n",
    "        for feature in columns:\n",
    "            le =  preprocessing.LabelEncoder()\n",
    "            df_test[feature] = le.fit_transform(df_test[feature])\n",
    "        X_test = df_test.to_numpy() \n",
    "        y_pred = self.NN.predict(X_test)\n",
    "        \n",
    "        #y_predDef = self.NNDef.predict(X_test)\n",
    "            \n",
    "        accuracy = accuracy_score(y_test,y_pred) * 100\n",
    "        #accuracyDef = accuracy_score(y_test,y_predDef) * 100\n",
    "        #print(\"Default Accuracy\")\n",
    "        #print(accuracyDef)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jV9onC4oytIg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/samangivian/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_size = 2000\n",
    "df_train = NN_load_train_data(df,classes, train_size)\n",
    "test_size = 1000\n",
    "df_test = NN_load_test_data(df , classes, train_size+1,train_size+1+test_size)\n",
    "\n",
    "accuracy_list = []\n",
    "neuron_list = []\n",
    "password_dic_length = {}\n",
    "entropy_dic = {}\n",
    "for r in range(0,len(df_train)):\n",
    "    password  = df_train.iloc[r]\n",
    "    if(password[\"entropy\"] not in entropy_dic ):\n",
    "        entropy_dic[password[\"entropy\"]] =password[\"entropy\"]\n",
    "    if(len(password[\"password\"]) not in password_dic_length):\n",
    "        password_dic_length[len(password[\"password\"])] = len(password[\"password\"])\n",
    "\n",
    "#print(len(entropy_dic))\n",
    "#print(len(password_dic_length))\n",
    "                                                             \n",
    "NNModel = NNClass()\n",
    "\n",
    "for layer1_neruon_count in range(1,101,20):\n",
    "    for layer2_neruon_count in range(1,101,20):\n",
    "        total = 0\n",
    "        trial_num = 8\n",
    "        for r in range(0,trial_num):\n",
    "            NNModel.model_generator(df_train,len(entropy_dic),len(password_dic_length))\n",
    "            total = total+ NNModel.accuracy(df_test)\n",
    "        acr = total/trial_num\n",
    "        neuron_list.append([layer1_neruon_count,layer2_neruon_count])\n",
    "        accuracy_list.append(acr)\n",
    "        #print(NNModel.accuracy(df_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy list for different number of neurons:\n",
      "[83.275, 83.125, 84.60000000000001, 86.3875, 85.175, 82.4125, 85.18750000000001, 84.43749999999999, 82.27499999999999, 85.5625, 84.3375, 84.23750000000001, 83.8, 84.32500000000002, 84.8, 85.8125, 85.625, 85.49999999999999, 84.75, 87.05000000000001, 85.94999999999999, 83.83749999999999, 83.225, 81.8625, 84.46249999999999]\n",
      "Maximum accuracy\n",
      "87.05000000000001\n",
      "Number of neurons in each layer of the maximum accuracy:\n",
      "[61, 81]\n"
     ]
    }
   ],
   "source": [
    "max_acr = (max(accuracy_list))\n",
    "\n",
    "print(\"Accuracy list for different number of neurons:\")\n",
    "print(accuracy_list)\n",
    "max_index = accuracy_list.index(max_acr)\n",
    "print(\"Maximum accuracy\")\n",
    "print(max_acr)\n",
    "print(\"Number of neurons in each layer of the maximum accuracy:\")\n",
    "print(neuron_list[max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWNjqQRX3073"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9bSOW0o329X"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rD37lYBoyRpd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0yIcO9Jx20K"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuNXa_gEyaQf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
